# Housing-in-Buenos-aires
Assignment: Predicting Apartment Prices in Mexico City ðŸ‡²ðŸ‡½

import warnings
â€‹
import wqet_grader
â€‹
warnings.simplefilter(action="ignore", category=FutureWarning)
wqet_grader.init("Project 2 Assessment")
Note: In this project there are graded tasks in both the lesson notebooks and in this assignment. Together they total 24 points. The minimum score you need to move to the next project is 22 points. Once you get 22 points, you will be enrolled automatically in the next project, and this assignment will be closed. This means that you might not be able to complete the last two tasks in this notebook. If you get an error message saying that you've already passed the course, that's good news. You can stop this assignment and move onto the project 3.
In this assignment, you'll decide which libraries you need to complete the tasks. You can import them in the cell below. ðŸ‘‡

# Import libraries here
â€‹
from glob import glob
â€‹
import plotly.express as px
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import wqet_grader
from category_encoders import OneHotEncoder
from IPython.display import VimeoVideo
from ipywidgets import Dropdown, FloatSlider, IntSlider, interact
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, Ridge  # noqa F401
from sklearn.metrics import mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.utils.validation import check_is_fitted
1. Prepare Data
1.1. Import
Task 2.5.1: (8 points) Write a wrangle function that takes the name of a CSV file as input and returns a DataFrame. The function should do the following steps:

Subset the data in the CSV file and return only apartments in Mexico City ("Distrito Federal") that cost less than $100,000.
Remove outliers by trimming the bottom and top 10% of properties in terms of "surface_covered_in_m2".
Create separate "lat" and "lon" columns.
Mexico City is divided into 16 boroughs. Create a "borough" feature from the "place_with_parent_names" column.
Drop columns that are more than 50% null values.
Drop columns containing low- or high-cardinality categorical values.
Drop any columns that would constitute leakage for the target "price_aprox_usd".
Drop any columns that would create issues of multicollinearity.
Tip: Don't try to satisfy all the criteria in the first version of your wrangle function. Instead, work iteratively. Start with the first criteria, test it out with one of the Mexico CSV files in the data/ directory, and submit it to the grader for feedback. Then add the next criteria.
# Build your `wrangle` function
def wrangle(filepath):
    # import data
    df = pd.read_csv(filepath)
    # subset to '"capital federal"'
    mask_ba = df['place_with_parent_names'].str.contains("Distrito Federal")
    # subset to '"apartments"'
    mask_apt = df['property_type'] == 'apartment'
    #subset to '"price aprox usd"' < 100,000
    mask_price = df['price_aprox_usd'] < 100_000
    df = df[mask_ba & mask_apt & mask_price] 
    
     # remove outliers by '" surface_in_m2"'
    low, high = df['surface_covered_in_m2'].quantile([0.1, 0.9])
    mask_area = df['surface_covered_in_m2'].between(low, high)
    df = df[mask_area]
    
     # subset to  split '" lat-lon"' col
    df[['lat','lon']] = df['lat-lon'].str.split(',', expand = True).astype(float)
    df.drop(columns = 'lat-lon', inplace = True )
    
    # creating '"borough"' feature
    df["borough"] = df["place_with_parent_names"].str.split('|',expand=True)[1]
    df.drop(columns = "place_with_parent_names", inplace=True )
    
     # drop features of high null counts
    df.drop(columns = ['floor','expenses'], inplace=True)
    
    # drop low - high cardinality categorical columns
    df.drop(columns = ['operation', 'property_type','currency','properati_url'], inplace=True)
    
    # drop leaky columns
    df.drop(columns = [
         'price',
         'price_aprox_local_currency',
         'price_per_m2',
         'price_usd_per_m2',
    ],
            inplace=True)
    
     # drop multicolliniearlty(highly correted features)
    df.drop(columns=['surface_total_in_m2', 'rooms'], inplace=True)
â€‹
    return df
# Use this cell to test your wrangle function and explore the data
df = wrangle("data/mexico-city-real-estate-1.csv")
print('df shape:', df.shape)
df.info()
â€‹
â€‹
df shape: (1101, 5)
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1101 entries, 11 to 4605
Data columns (total 5 columns):
 #   Column                 Non-Null Count  Dtype  
---  ------                 --------------  -----  
 0   price_aprox_usd        1101 non-null   float64
 1   surface_covered_in_m2  1101 non-null   float64
 2   lat                    1041 non-null   float64
 3   lon                    1041 non-null   float64
 4   borough                1101 non-null   object 
dtypes: float64(4), object(1)
memory usage: 51.6+ KB
â€‹
wqet_grader.grade(
    "Project 2 Assessment", "Task 2.5.1", wrangle("data/mexico-city-real-estate-1.csv")
)
Yes! Your hard work is paying off.

Score: 1

Task 2.5.2: Use glob to create the list files. It should contain the filenames of all the Mexico City real estate CSVs in the ./data directory, except for mexico-city-test-features.csv.

files = glob("data/mexico-city-real-estate-*.csv")
files
['data/mexico-city-real-estate-3.csv',
 'data/mexico-city-real-estate-5.csv',
 'data/mexico-city-real-estate-1.csv',
 'data/mexico-city-real-estate-2.csv',
 'data/mexico-city-real-estate-4.csv']
wqet_grader.grade("Project 2 Assessment", "Task 2.5.2", files)
Yes! Your hard work is paying off.

Score: 1

Task 2.5.3: Combine your wrangle function, a list comprehension, and pd.concat to create a DataFrame df. It should contain all the properties from the five CSVs in files.

df = pd.concat([wrangle(file)for file in files], ignore_index=True)
print(df.info())
df.head()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5473 entries, 0 to 5472
Data columns (total 5 columns):
 #   Column                 Non-Null Count  Dtype  
---  ------                 --------------  -----  
 0   price_aprox_usd        5473 non-null   float64
 1   surface_covered_in_m2  5473 non-null   float64
 2   lat                    5149 non-null   float64
 3   lon                    5149 non-null   float64
 4   borough                5473 non-null   object 
dtypes: float64(4), object(1)
memory usage: 213.9+ KB
None
price_aprox_usd	surface_covered_in_m2	lat	lon	borough
0	29315.91	65.0	19.401350	-99.114726	Iztacalco
1	8693.26	48.0	19.283413	-99.055060	TlÃ¡huac
2	70801.04	80.0	19.484703	-99.211365	Azcapotzalco
3	72250.71	60.0	19.403504	-99.154502	Benito JuÃ¡rez
4	68492.42	70.0	19.364993	-99.155739	Benito JuÃ¡rez
â€‹
wqet_grader.grade("Project 2 Assessment", "Task 2.5.3", df)
Wow, you're making great progress.

Score: 1

1.2. Explore
Task 2.5.4: Create a histogram showing the distribution of apartment prices ("price_aprox_usd") in df. Be sure to label the x-axis "Area [sq meters]", the y-axis "Count", and give it the title "Distribution of Apartment Prices".

What does the distribution of price look like? Is the data normal, a little skewed, or very skewed?

# Plot distribution of price
plt.hist(df["price_aprox_usd"])
plt.xlabel("Area [sq meters]")
plt.ylabel("Count")
plt.title("Distribution of Apartment Prices");
# Don't delete the code below ðŸ‘‡
plt.savefig("images/2-5-4.png", dpi=150)
â€‹

with open("images/2-5-4.png", "rb") as file:
    wqet_grader.grade("Project 2 Assessment", "Task 2.5.4", file)
That's the right answer. Keep it up!

Score: 1

Task 2.5.5: Create a scatter plot that shows apartment price ("price_aprox_usd") as a function of apartment size ("surface_covered_in_m2"). Be sure to label your axes "Price [USD]" and "Area [sq meters]", respectively. Your plot should have the title "Mexico City: Price vs. Area".

Do you see a relationship between price and area in the data? How is this similar to or different from the Buenos Aires dataset?

# Plot price vs area
plt.scatter(x = df['surface_covered_in_m2'], y = df['price_aprox_usd'])
plt.xlabel("Area [sq meters]")
plt.ylabel("Price [USD]")
plt.title("Mexico City: Price vs. Area");
# Don't delete the code below ðŸ‘‡
plt.savefig("images/2-5-5.png", dpi=150)
â€‹

with open("images/2-5-5.png", "rb") as file:
    wqet_grader.grade("Project 2 Assessment", "Task 2.5.5", file)
Awesome work.

Score: 1

Task 2.5.6: (UNGRADED) Create a Mapbox scatter plot that shows the location of the apartments in your dataset and represent their price using color.

What areas of the city seem to have higher real estate prices?

# Plot Mapbox location and price
fig = px.scatter_mapbox(
    df,  # Our DataFrame
    lat='lat',
    lon='lon',
    width=600,  # Width of map
    height=600,  # Height of map
    color="price_aprox_usd",
    hover_data=["price_aprox_usd"],  # Display price when hovering mouse over house
)
â€‹
fig.update_layout(mapbox_style="open-street-map")
â€‹
fig.show()

1.3. Split
Task 2.5.7: Create your feature matrix X_train and target vector y_train. Your target is "price_aprox_usd". Your features should be all the columns that remain in the DataFrame you cleaned above.

# Split data into feature matrix `X_train` and target vector `y_train`.
features = ['surface_covered_in_m2','lat','lon','borough']
target = "price_aprox_usd"
X_train = df[features]
y_train = df[target]
â€‹
wqet_grader.grade("Project 2 Assessment", "Task 2.5.7a", X_train)
ðŸ¥³

Score: 1

â€‹
wqet_grader.grade("Project 2 Assessment", "Task 2.5.7b", y_train)
Very impressive.

Score: 1

2. Build Model
2.1. Baseline
Task 2.5.8: Calculate the baseline mean absolute error for your model.

y_mean = y_train.mean()
y_pred_baseline = [y_mean] * len(y_train)
baseline_mae = mean_absolute_error(y_train, y_pred_baseline)
print("Mean apt price:", y_mean)
print("Baseline MAE:", baseline_mae)
Mean apt price: 54246.53149826441
Baseline MAE: 17239.939475888317
wqet_grader.grade("Project 2 Assessment", "Task 2.5.8", [baseline_mae])
Good work!

Score: 1

2.2. Iterate
Task 2.5.9: Create a pipeline named model that contains all the transformers necessary for this dataset and one of the predictors you've used during this project. Then fit your model to the training data.

# Build Model
model = make_pipeline(
    OneHotEncoder(),
    SimpleImputer(),
    Ridge()
)
# Fit model
model.fit(X_train, y_train)
Pipeline(steps=[('onehotencoder', OneHotEncoder(cols=['borough'])),
                ('simpleimputer', SimpleImputer()), ('ridge', Ridge())])
â€‹
wqet_grader.grade("Project 2 Assessment", "Task 2.5.9", model)
Wow, you're making great progress.

Score: 1

2.3. Evaluate
Task 2.5.10: Read the CSV file mexico-city-test-features.csv into the DataFrame X_test.

Tip: Make sure the X_train you used to train your model has the same column order as X_test. Otherwise, it may hurt your model's performance.
X_test = pd.read_csv('data/mexico-city-test-features.csv')
print(X_test.info())
X_test.head()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1041 entries, 0 to 1040
Data columns (total 4 columns):
 #   Column                 Non-Null Count  Dtype  
---  ------                 --------------  -----  
 0   surface_covered_in_m2  1041 non-null   float64
 1   lat                    986 non-null    float64
 2   lon                    986 non-null    float64
 3   borough                1041 non-null   object 
dtypes: float64(3), object(1)
memory usage: 32.7+ KB
None
surface_covered_in_m2	lat	lon	borough
0	60.0	19.493185	-99.205755	Azcapotzalco
1	55.0	19.307247	-99.166700	CoyoacÃ¡n
2	50.0	19.363469	-99.010141	Iztapalapa
3	60.0	19.474655	-99.189277	Azcapotzalco
4	74.0	19.394628	-99.143842	Benito JuÃ¡rez
â€‹
wqet_grader.grade("Project 2 Assessment", "Task 2.5.10", X_test)
Way to go!

Score: 1

Task 2.5.11: Use your model to generate a Series of predictions for X_test. When you submit your predictions to the grader, it will calculate the mean absolute error for your model.

y_test_pred = pd.Series(model.predict(X_test))
y_test_pred.head()
0    53538.366480
1    53171.988369
2    34263.884179
3    53488.425607
4    68738.924884
dtype: float64
wqet_grader.grade("Project 2 Assessment", "Task 2.5.11", y_test_pred)
Your model's mean absolute error is 14901.618. Correct.

Score: 1

3. Communicate Results
Task 2.5.12: Create a Series named feat_imp. The index should contain the names of all the features your model considers when making predictions; the values should be the coefficient values associated with each feature. The Series should be sorted ascending by absolute value.

coefficients = model.named_steps['ridge'].coef_
features = model.named_steps['onehotencoder'].get_feature_names()
feat_imp = pd.Series(coefficients, index=features)
feat_imp
surface_covered_in_m2      291.654156
lat                        478.901375
lon                      -2492.221814
borough_1                  405.403127
borough_2               -14166.869486
borough_3                 2459.288646
borough_4                13778.188880
borough_5                 1977.314718
borough_6                -5609.918629
borough_7                 -350.531990
borough_8                 3275.121061
borough_9                -6637.429757
borough_10                3737.561001
borough_11              -13349.017448
borough_12               10319.429804
borough_13                 929.857400
borough_14               -5925.666450
borough_15                9157.269123
dtype: float64
â€‹
wqet_grader.grade("Project 2 Assessment", "Task 2.5.13", feat_imp)
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
Input In [44], in <cell line: 1>()
----> 1 wqet_grader.grade("Project 2 Assessment", "Task 2.5.13", feat_imp)

File /opt/conda/lib/python3.9/site-packages/wqet_grader/__init__.py:180, in grade(assessment_id, question_id, submission)
    175 def grade(assessment_id, question_id, submission):
    176   submission_object = {
    177     'type': 'simple',
    178     'argument': [submission]
    179   }
--> 180   return show_score(grade_submission(assessment_id, question_id, submission_object))

File /opt/conda/lib/python3.9/site-packages/wqet_grader/transport.py:143, in grade_submission(assessment_id, question_id, submission_object)
    141     raise Exception('Grader raised error: {}'.format(error['message']))
    142   else:
--> 143     raise Exception('Could not grade submission: {}'.format(error['message']))
    144 result = envelope['data']['result']
    146 # Used only in testing

Exception: Could not grade submission: Could not verify access to this assessment: Received error from WQET submission API: You have already passed this course!
Task 2.5.13: Create a horizontal bar chart that shows the 10 most influential coefficients for your model. Be sure to label your x- and y-axis "Importance [USD]" and "Feature", respectively, and give your chart the title "Feature Importances for Apartment Price".

# Create horizontal bar chart
feat_imp.sort_values(key=abs).tail(10).plot(kind='barh')
plt.xlabel('Importance [USD]')
plt.ylabel('Feature')
plt.title('Feature Importance for Apartment Price')
# Don't delete the code below ðŸ‘‡
plt.savefig("images/2-5-14.png", dpi=150)
â€‹
with open("images/2-5-14.png", "rb") as file:
    wqet_grader.grade("Project 2 Assessment", "Task 2.5.14", file)
